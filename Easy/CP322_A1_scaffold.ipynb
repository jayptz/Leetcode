{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bc8273",
   "metadata": {},
   "source": [
    "\n",
    "# CP322 Machine Learning — Assignment 1 Scaffold\n",
    "\n",
    "**Name:** _Your Name Here_  \n",
    "**Course:** CP322 — Fall 2025  \n",
    "**Dataset:** NYC Airbnb Open Data (price as target)\n",
    "\n",
    "This notebook is organized to match the assignment sections exactly.  \n",
    "Replace `_TODO_` blocks, run cells top-to-bottom, and add short markdown reflections where prompted.\n",
    "\n",
    "> Tip: Keep outputs (tables/plots) visible. For code cells, write brief comments explaining your choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c536714a",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d6cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML tools\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Mapping\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', 200)\n",
    "plt.rcParams['figure.figsize'] = (8, 5)\n",
    "\n",
    "print(\"✅ Libraries loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c9867f",
   "metadata": {},
   "source": [
    "## 0.1) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47adc9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Update the path to your CSV (e.g., 'AB_NYC_2019.csv')\n",
    "CSV_PATH = \"AB_NYC_2019.csv\"  # _TODO_: set the correct path\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"Loaded: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91724da1",
   "metadata": {},
   "source": [
    "## 1) Exploratory Data Analysis (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54b2177",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Descriptive statistics\n",
    "- Show mean, median, std for all numeric features.\n",
    "- Add a 2–3 sentence note on any obvious skew or spread.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "desc = df[num_cols].describe().T\n",
    "# Add median (50% already present) for emphasis if desired\n",
    "display(desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187e75bc",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 Handle missing values (per spec)\n",
    "- Drop rows where `reviews_per_month` is missing.\n",
    "- Fill missing `last_review` with `\"Unknown\"`.\n",
    "- Median imputation for numeric, mode for categorical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec7333",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows missing 'reviews_per_month'\n",
    "if 'reviews_per_month' in df.columns:\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=['reviews_per_month'])\n",
    "    print(f\"Dropped {before - len(df)} rows with missing reviews_per_month.\")\n",
    "else:\n",
    "    print(\"reviews_per_month column not found; skipping drop.\")\n",
    "\n",
    "# Coerce last_review to string and fill\n",
    "if 'last_review' in df.columns:\n",
    "    df['last_review'] = df['last_review'].astype('string').fillna('Unknown')\n",
    "else:\n",
    "    print(\"last_review column not found; skipping fill.\")\n",
    "\n",
    "# Median/mode imputation\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "for c in numeric_cols:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = df[c].fillna(df[c].median())\n",
    "\n",
    "for c in categorical_cols:\n",
    "    if df[c].isna().any():\n",
    "        mode = df[c].mode(dropna=True)\n",
    "        fill_val = mode.iloc[0] if len(mode) else \"Unknown\"\n",
    "        df[c] = df[c].fillna(fill_val)\n",
    "\n",
    "print(\"Missing values handled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2c1d7",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3 Outliers in `price` via IQR\n",
    "Compute IQR = Q3 − Q1. Drop rows with price outside `[Q1 − 1.5×IQR, Q3 + 1.5×IQR]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93204356",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert 'price' in df.columns, \"Expected a 'price' column\"\n",
    "Q1, Q3 = df['price'].quantile([0.25, 0.75])\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "before = len(df)\n",
    "df = df[(df['price'] >= lower) & (df['price'] <= upper)]\n",
    "print(f\"Removed {before - len(df)} outlier rows by price IQR. Bounds: [{lower:.2f}, {upper:.2f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0069f136",
   "metadata": {},
   "source": [
    "\n",
    "### 1.4 Visualizations\n",
    "- Histogram of price  \n",
    "- Scatter: price vs number_of_reviews  \n",
    "- Boxplot: price by room_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3051363",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Histogram\n",
    "ax = df['price'].plot(kind='hist', bins=50, title='Histogram of Price')\n",
    "ax.set_xlabel('Price')\n",
    "plt.show()\n",
    "\n",
    "# Scatter\n",
    "if 'number_of_reviews' in df.columns:\n",
    "    plt.scatter(df['number_of_reviews'], df['price'], alpha=0.3)\n",
    "    plt.title('Price vs Number of Reviews')\n",
    "    plt.xlabel('Number of Reviews')\n",
    "    plt.ylabel('Price')\n",
    "    plt.show()\n",
    "\n",
    "# Boxplot by room_type\n",
    "if 'room_type' in df.columns:\n",
    "    sns.boxplot(data=df, x='room_type', y='price')\n",
    "    plt.title('Price by Room Type')\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb0793",
   "metadata": {},
   "source": [
    "\n",
    "### 1.5 Correlation heatmap + short discussion\n",
    "Focus columns: `price, minimum_nights, number_of_reviews, reviews_per_month, calculated_host_listings_count, availability_365`.\n",
    "Briefly discuss any strong correlations (multicollinearity) you observe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a20214",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_features = ['price', 'minimum_nights', 'number_of_reviews', 'reviews_per_month',\n",
    "                 'calculated_host_listings_count', 'availability_365']\n",
    "existing = [c for c in corr_features if c in df.columns]\n",
    "corr = df[existing].corr(numeric_only=True)\n",
    "\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap (Selected Features)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6226af08",
   "metadata": {},
   "source": [
    "\n",
    "_Notes on multicollinearity:_  \n",
    "- _TODO: add 2–4 bullets on any pairs with high |corr|, and whether that might impact linear models._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d58fe3",
   "metadata": {},
   "source": [
    "## 2) Feature Engineering (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779a7fd8",
   "metadata": {},
   "source": [
    "\n",
    "- One-hot encode `neighbourhood_group` and `room_type`.  \n",
    "- Create toy feature: `price_per_accommodates = price / minimum_nights` (name from spec; see note).  \n",
    "- Scale numeric: `minimum_nights, number_of_reviews, reviews_per_month, calculated_host_listings_count, availability_365, price_per_accommodates`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3684a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure required columns exist (gracefully skip otherwise)\n",
    "cat_cols_needed = [col for col in ['neighbourhood_group', 'room_type'] if col in df.columns]\n",
    "\n",
    "# Derived feature\n",
    "if 'price' in df.columns and 'minimum_nights' in df.columns:\n",
    "    df['price_per_accommodates'] = df['price'] / df['minimum_nights'].replace(0, np.nan)\n",
    "    df['price_per_accommodates'] = df['price_per_accommodates'].fillna(0)\n",
    "else:\n",
    "    print(\"Warning: missing 'price' or 'minimum_nights'; skipping derived feature.\")\n",
    "\n",
    "# Columns for later scaling\n",
    "num_for_scaler = [c for c in [\n",
    "    'minimum_nights', 'number_of_reviews', 'reviews_per_month',\n",
    "    'calculated_host_listings_count', 'availability_365', 'price_per_accommodates'\n",
    "] if c in df.columns]\n",
    "\n",
    "print(\"Categorical to encode:\", cat_cols_needed)\n",
    "print(\"Numeric to scale:\", num_for_scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239cc7fb",
   "metadata": {},
   "source": [
    "## 3) Baseline Regression Models (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a8a395",
   "metadata": {},
   "source": [
    "\n",
    "- Split 80/20.  \n",
    "- Baseline Linear Regression with features: `neighbourhood_group, room_type, minimum_nights, number_of_reviews, reviews_per_month, calculated_host_listings_count, availability_365`.  \n",
    "- Report RMSE, MSE, R².\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c905e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_cols = [c for c in [\n",
    "    'neighbourhood_group', 'room_type',\n",
    "    'minimum_nights', 'number_of_reviews', 'reviews_per_month',\n",
    "    'calculated_host_listings_count', 'availability_365'\n",
    "] if c in df.columns]\n",
    "\n",
    "target_col = 'price'\n",
    "assert target_col in df.columns, \"Missing target 'price'.\"\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "cat_cols = [c for c in feature_cols if X[c].dtype == 'object' or str(X[c].dtype).startswith('string')]\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(transformers=[\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols),\n",
    "    ('num', StandardScaler(), num_cols)\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "linreg = Pipeline(steps=[\n",
    "    ('prep', preprocess),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "linreg.fit(X_train, y_train)\n",
    "y_pred = linreg.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mse = mean_squared_error(y_test, y_pred, squared=True)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Baseline Linear Regression — RMSE: {rmse:.2f}, MSE: {mse:.2f}, R²: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a905427",
   "metadata": {},
   "source": [
    "## 4) Lasso, Ridge, and ElasticNet Regression (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef2e737",
   "metadata": {},
   "source": [
    "\n",
    "- Grid search with 5-fold CV.  \n",
    "- Compare RMSE, MSE, R² on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2596ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alphas = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "def eval_model(pipeline, name):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    pred = pipeline.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, pred, squared=False)\n",
    "    mse = mean_squared_error(y_test, pred, squared=True)\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    print(f\"{name:>10s} → RMSE: {rmse:.2f}, MSE: {mse:.2f}, R²: {r2:.3f}\")\n",
    "    return {'model': name, 'rmse': rmse, 'mse': mse, 'r2': r2}\n",
    "\n",
    "# Lasso\n",
    "lasso = Pipeline([('prep', preprocess), ('model', Lasso(max_iter=10000))])\n",
    "gs_lasso = GridSearchCV(lasso, param_grid={'model__alpha': alphas}, cv=5, n_jobs=-1)\n",
    "res_lasso = eval_model(gs_lasso, \"Lasso\")\n",
    "\n",
    "# Ridge\n",
    "ridge = Pipeline([('prep', preprocess), ('model', Ridge())])\n",
    "gs_ridge = GridSearchCV(ridge, param_grid={'model__alpha': alphas}, cv=5, n_jobs=-1)\n",
    "res_ridge = eval_model(gs_ridge, \"Ridge\")\n",
    "\n",
    "# ElasticNet\n",
    "enet = Pipeline([('prep', preprocess), ('model', ElasticNet(max_iter=10000))])\n",
    "enet_grid = {'model__alpha': alphas, 'model__l1_ratio': [0.1, 0.5, 0.9]}\n",
    "gs_enet = GridSearchCV(enet, param_grid=enet_grid, cv=5, n_jobs=-1)\n",
    "res_enet = eval_model(gs_enet, \"ElasticNet\")\n",
    "\n",
    "results_df = pd.DataFrame([res_lasso, res_ridge, res_enet])\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd398b15",
   "metadata": {},
   "source": [
    "## 5) Bias–Variance Tradeoff & Model Complexity (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390574d0",
   "metadata": {},
   "source": [
    "\n",
    "- Polynomial features with degrees 1, 2, 3, 4 (on numeric predictors).  \n",
    "- Plot learning curves (training vs validation error).  \n",
    "- Write 5–7 sentences discussing bias/variance, overfitting, underfitting trends you see.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e70ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use only numeric predictors for PolynomialFeatures\n",
    "num_only = [c for c in num_cols]  # from earlier split cell\n",
    "assert len(num_only) > 0, \"Need numeric predictors for polynomial features.\"\n",
    "\n",
    "def poly_pipeline(deg):\n",
    "    num_pipe = Pipeline([('poly', PolynomialFeatures(degree=deg, include_bias=False)),\n",
    "                         ('sc', StandardScaler())])\n",
    "    # Transform only numeric columns; passthrough categoricals via one-hot\n",
    "    poly_ct = ColumnTransformer([\n",
    "        ('num', num_pipe, num_only),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "    ])\n",
    "    return Pipeline([('prep', poly_ct), ('model', LinearRegression())])\n",
    "\n",
    "degrees = [1, 2, 3, 4]\n",
    "poly_scores = []\n",
    "\n",
    "for d in degrees:\n",
    "    pipe = poly_pipeline(d)\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator=pipe,\n",
    "        X=X, y=y,\n",
    "        cv=5,\n",
    "        train_sizes=np.linspace(0.2, 1.0, 5),\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    train_rmse = np.sqrt(-train_scores)\n",
    "    val_rmse = np.sqrt(-val_scores)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_rmse.mean(axis=1), marker='o', label='Train RMSE')\n",
    "    plt.plot(train_sizes, val_rmse.mean(axis=1), marker='s', label='Validation RMSE')\n",
    "    plt.title(f'Learning Curve (Degree {d})')\n",
    "    plt.xlabel('Training Samples')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Final fit for test performance\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    preds = pipe.predict(X_te)\n",
    "    rmse = mean_squared_error(y_te, preds, squared=False)\n",
    "    r2 = r2_score(y_te, preds)\n",
    "    poly_scores.append({'degree': d, 'rmse': rmse, 'r2': r2})\n",
    "\n",
    "pd.DataFrame(poly_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e1300",
   "metadata": {},
   "source": [
    "\n",
    "_Discussion (bias–variance):_  \n",
    "- _TODO: Summarize how RMSE changes with degree. Indicate where you see underfitting (high bias) vs overfitting (high variance)._  \n",
    "- _Note any sweet spot (degree) and speculate why given the dataset._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa22a4",
   "metadata": {},
   "source": [
    "## 6) Advanced Visualization (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eea7a0",
   "metadata": {},
   "source": [
    "\n",
    "### 6.1 Geographic heatmap of prices\n",
    "- Requires `latitude`, `longitude`, and `price`.  \n",
    "- Renders an interactive map (may not display in some hosted graders; include a screenshot if needed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21629666",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if all(col in df.columns for col in ['latitude', 'longitude', 'price']):\n",
    "    m = folium.Map(location=[40.7128, -74.0060], zoom_start=10, tiles='cartodbpositron')  # NYC center\n",
    "    heat_data = df[['latitude', 'longitude', 'price']].dropna().values.tolist()\n",
    "    HeatMap(heat_data, radius=8, blur=15, max_zoom=13).add_to(m)\n",
    "    m\n",
    "else:\n",
    "    print(\"latitude/longitude/price not found; skipping heatmap.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62ed95d",
   "metadata": {},
   "source": [
    "\n",
    "### 6.2 Predicted vs Actual (best model)\n",
    "- Refit the best-performing model from Section 4 on train, evaluate on test, and plot predictions vs actual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose the best by lowest RMSE in results_df (from Section 4)\n",
    "best_row = results_df.sort_values('rmse', ascending=True).iloc[0]\n",
    "best_name = best_row['model']\n",
    "print(\"Best model from Section 4:\", best_name)\n",
    "\n",
    "best_estimator = {'Lasso': gs_lasso, 'Ridge': gs_ridge, 'ElasticNet': gs_enet}[best_name]\n",
    "best_estimator.fit(X_train, y_train)\n",
    "preds = best_estimator.predict(X_test)\n",
    "\n",
    "plt.scatter(y_test, preds, alpha=0.4)\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title(f'Predicted vs Actual — {best_name}')\n",
    "# 45-degree reference line\n",
    "lims = [min(plt.xlim()[0], plt.ylim()[0]), max(plt.xlim()[1], plt.ylim()[1])]\n",
    "plt.plot(lims, lims, '--')\n",
    "plt.xlim(lims); plt.ylim(lims)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a104e",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e3097f",
   "metadata": {},
   "source": [
    "\n",
    "- Briefly list any assumptions.  \n",
    "- Note any data cleaning deviations (and why).  \n",
    "- Cite data source and libraries.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}