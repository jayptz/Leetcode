{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f605ce4",
   "metadata": {},
   "source": [
    "\n",
    "# CP322 Machine Learning — Assignment 1 (Completed Code)\n",
    "\n",
    "**Name:** _Your Name Here_  \n",
    "**Course:** CP322 — Fall 2025  \n",
    "**Dataset:** NYC Airbnb Open Data (price as target)\n",
    "\n",
    "> Set `CSV_PATH` below to your dataset file (e.g., `AB_NYC_2019.csv`) and run the notebook top-to-bottom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e649d2e",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffae620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML tools\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Mapping\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Utility\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "plt.rcParams['figure.figsize'] = (8, 5)\n",
    "\n",
    "def say(msg: str):\n",
    "    print(f\"✨ {msg}\")\n",
    "\n",
    "print(\"✅ Libraries loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f437a81",
   "metadata": {},
   "source": [
    "## 0.1) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47549626",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Set this to your CSV path\n",
    "CSV_PATH = \"AB_NYC_2019.csv\"  # <-- change to the actual path\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "say(f\"Loaded: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae34a16",
   "metadata": {},
   "source": [
    "## 1) Exploratory Data Analysis (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a9a372",
   "metadata": {},
   "source": [
    "### 1.1 Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c14637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "desc = df[num_cols].describe().T\n",
    "display(desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e456d",
   "metadata": {},
   "source": [
    "### 1.2 Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows where reviews_per_month is missing\n",
    "if 'reviews_per_month' in df.columns:\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=['reviews_per_month'])\n",
    "    say(f\"Dropped {before - len(df)} rows with missing reviews_per_month.\")\n",
    "else:\n",
    "    say(\"No 'reviews_per_month' column found; skipping drop.\")\n",
    "\n",
    "# Replace missing dates in last_review with 'Unknown'\n",
    "if 'last_review' in df.columns:\n",
    "    df['last_review'] = df['last_review'].astype('string').fillna('Unknown')\n",
    "    say(\"Filled missing 'last_review' with 'Unknown' and coerced to string.\")\n",
    "else:\n",
    "    say(\"No 'last_review' column found; skipping fill.\")\n",
    "\n",
    "# Median for numerics / Mode for categoricals\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "num_filled = 0\n",
    "for c in numeric_cols:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = df[c].fillna(df[c].median())\n",
    "        num_filled += 1\n",
    "\n",
    "cat_filled = 0\n",
    "for c in categorical_cols:\n",
    "    if df[c].isna().any():\n",
    "        mode = df[c].mode(dropna=True)\n",
    "        fill_val = mode.iloc[0] if len(mode) else \"Unknown\"\n",
    "        df[c] = df[c].fillna(fill_val)\n",
    "        cat_filled += 1\n",
    "\n",
    "say(f\"Imputed {num_filled} numeric and {cat_filled} categorical columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c3dcb",
   "metadata": {},
   "source": [
    "### 1.3 Detect & remove price outliers with IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert 'price' in df.columns, \"Expected a 'price' column in dataset.\"\n",
    "Q1, Q3 = df['price'].quantile([0.25, 0.75])\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "before = len(df)\n",
    "df = df[(df['price'] >= lower) & (df['price'] <= upper)]\n",
    "say(f\"IQR filter on price: bounds [{lower:.2f}, {upper:.2f}]. Removed {before - len(df)} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf4f429",
   "metadata": {},
   "source": [
    "### 1.4 Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Histogram of price\n",
    "ax = df['price'].plot(kind='hist', bins=50, title='Histogram of Price')\n",
    "ax.set_xlabel('Price')\n",
    "plt.show()\n",
    "\n",
    "# Scatter: price vs number_of_reviews\n",
    "if 'number_of_reviews' in df.columns:\n",
    "    plt.scatter(df['number_of_reviews'], df['price'], alpha=0.3)\n",
    "    plt.title('Price vs Number of Reviews')\n",
    "    plt.xlabel('Number of Reviews')\n",
    "    plt.ylabel('Price')\n",
    "    plt.show()\n",
    "\n",
    "# Boxplots: price by room_type\n",
    "if 'room_type' in df.columns:\n",
    "    sns.boxplot(data=df, x='room_type', y='price')\n",
    "    plt.title('Price by Room Type')\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7760b851",
   "metadata": {},
   "source": [
    "### 1.5 Correlation heatmap + brief discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa19ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_features = ['price', 'minimum_nights', 'number_of_reviews', 'reviews_per_month',\n",
    "                 'calculated_host_listings_count', 'availability_365']\n",
    "existing = [c for c in corr_features if c in df.columns]\n",
    "corr = df[existing].corr(numeric_only=True)\n",
    "\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap (Selected Features)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Notes: discuss any strong/weak correlations and potential multicollinearity.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e930c348",
   "metadata": {},
   "source": [
    "## 2) Feature Engineering (3 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b34551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One-hot encode neighbourhood_group and room_type later via ColumnTransformer\n",
    "cat_cols_needed = [col for col in ['neighbourhood_group', 'room_type'] if col in df.columns]\n",
    "\n",
    "# Derived feature: price_per_accommodates (price / minimum_nights)\n",
    "if 'price' in df.columns and 'minimum_nights' in df.columns:\n",
    "    df['price_per_accommodates'] = df['price'] / df['minimum_nights'].replace(0, np.nan)\n",
    "    df['price_per_accommodates'] = df['price_per_accommodates'].fillna(0)\n",
    "    say(\"Created derived feature 'price_per_accommodates'.\")\n",
    "else:\n",
    "    say(\"Missing 'price' or 'minimum_nights'; skipping derived feature.\")\n",
    "\n",
    "num_for_scaler = [c for c in [\n",
    "    'minimum_nights', 'number_of_reviews', 'reviews_per_month',\n",
    "    'calculated_host_listings_count', 'availability_365', 'price_per_accommodates'\n",
    "] if c in df.columns]\n",
    "\n",
    "say(f\"Categoricals to encode later: {cat_cols_needed}\")\n",
    "say(f\"Numerics to scale later: {num_for_scaler}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37fb0a7",
   "metadata": {},
   "source": [
    "## 3) Baseline Regression Models (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe5fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_cols = [c for c in [\n",
    "    'neighbourhood_group', 'room_type',\n",
    "    'minimum_nights', 'number_of_reviews', 'reviews_per_month',\n",
    "    'calculated_host_listings_count', 'availability_365'\n",
    "] if c in df.columns]\n",
    "\n",
    "target_col = 'price'\n",
    "assert target_col in df.columns, \"Missing target 'price'.\"\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "cat_cols = [c for c in feature_cols if X[c].dtype == 'object' or str(X[c].dtype).startswith('string')]\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(transformers=[\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols),\n",
    "    ('num', StandardScaler(), num_cols)\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "linreg = Pipeline(steps=[\n",
    "    ('prep', preprocess),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "linreg.fit(X_train, y_train)\n",
    "y_pred = linreg.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mse = mean_squared_error(y_test, y_pred, squared=True)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "say(f\"Baseline Linear Regression — RMSE: {rmse:.2f}, MSE: {mse:.2f}, R²: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce5dd9",
   "metadata": {},
   "source": [
    "## 4) Lasso, Ridge, and ElasticNet Regression (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alphas = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "def eval_model(pipeline, name):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    pred = pipeline.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, pred, squared=False)\n",
    "    mse = mean_squared_error(y_test, pred, squared=True)\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    say(f\"{name} → RMSE: {rmse:.2f}, MSE: {mse:.2f}, R²: {r2:.3f}\")\n",
    "    return {'model': name, 'rmse': rmse, 'mse': mse, 'r2': r2}\n",
    "\n",
    "# Lasso\n",
    "lasso = Pipeline([('prep', preprocess), ('model', Lasso(max_iter=10000))])\n",
    "gs_lasso = GridSearchCV(lasso, param_grid={'model__alpha': alphas}, cv=5, n_jobs=-1)\n",
    "res_lasso = eval_model(gs_lasso, \"Lasso\")\n",
    "\n",
    "# Ridge\n",
    "ridge = Pipeline([('prep', preprocess), ('model', Ridge())])\n",
    "gs_ridge = GridSearchCV(ridge, param_grid={'model__alpha': alphas}, cv=5, n_jobs=-1)\n",
    "res_ridge = eval_model(gs_ridge, \"Ridge\")\n",
    "\n",
    "# ElasticNet\n",
    "enet = Pipeline([('prep', preprocess), ('model', ElasticNet(max_iter=10000))])\n",
    "enet_grid = {'model__alpha': alphas, 'model__l1_ratio': [0.1, 0.5, 0.9]}\n",
    "gs_enet = GridSearchCV(enet, param_grid=enet_grid, cv=5, n_jobs=-1)\n",
    "res_enet = eval_model(gs_enet, \"ElasticNet\")\n",
    "\n",
    "results_df = pd.DataFrame([res_lasso, res_ridge, res_enet]).sort_values('rmse')\n",
    "display(results_df.reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb288a",
   "metadata": {},
   "source": [
    "## 5) Bias–Variance Tradeoff & Model Complexity (3 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deed2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare lists again (from baseline step)\n",
    "cat_cols = [c for c in feature_cols if X[c].dtype == 'object' or str(X[c].dtype).startswith('string')]\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "assert len(num_cols) > 0, \"Need numeric predictors for polynomial features.\"\n",
    "\n",
    "def poly_pipeline(deg):\n",
    "    num_pipe = Pipeline([('poly', PolynomialFeatures(degree=deg, include_bias=False)),\n",
    "                         ('sc', StandardScaler())])\n",
    "    poly_ct = ColumnTransformer([\n",
    "        ('num', num_pipe, num_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "    ])\n",
    "    return Pipeline([('prep', poly_ct), ('model', LinearRegression())])\n",
    "\n",
    "degrees = [1, 2, 3, 4]\n",
    "poly_scores = []\n",
    "\n",
    "for d in degrees:\n",
    "    pipe = poly_pipeline(d)\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator=pipe,\n",
    "        X=X, y=y,\n",
    "        cv=5,\n",
    "        train_sizes=np.linspace(0.2, 1.0, 5),\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    train_rmse = np.sqrt(-train_scores)\n",
    "    val_rmse = np.sqrt(-val_scores)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_rmse.mean(axis=1), marker='o', label='Train RMSE')\n",
    "    plt.plot(train_sizes, val_rmse.mean(axis=1), marker='s', label='Validation RMSE')\n",
    "    plt.title(f'Learning Curve (Degree {d})')\n",
    "    plt.xlabel('Training Samples')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Final fit for test performance\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    preds = pipe.predict(X_te)\n",
    "    rmse = mean_squared_error(y_te, preds, squared=False)\n",
    "    r2 = r2_score(y_te, preds)\n",
    "    poly_scores.append({'degree': d, 'rmse': rmse, 'r2': r2})\n",
    "\n",
    "poly_df = pd.DataFrame(poly_scores).sort_values('degree')\n",
    "display(poly_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702fd227",
   "metadata": {},
   "source": [
    "\n",
    "**Discussion (Bias–Variance):**  \n",
    "- Comment on where the model underfits (high RMSE on train & val at low degree).  \n",
    "- Comment on where the model overfits (train RMSE drops while val RMSE worsens at high degree).  \n",
    "- Identify the degree with best validation/test RMSE and briefly justify.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68fa87",
   "metadata": {},
   "source": [
    "## 6) Advanced Visualization (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e96ef2",
   "metadata": {},
   "source": [
    "### 6.1 Geographic price heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfe4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if all(col in df.columns for col in ['latitude', 'longitude', 'price']):\n",
    "    m = folium.Map(location=[40.7128, -74.0060], zoom_start=10, tiles='cartodbpositron')\n",
    "    heat_data = df[['latitude', 'longitude', 'price']].dropna().values.tolist()\n",
    "    HeatMap(heat_data, radius=8, blur=15, max_zoom=13).add_to(m)\n",
    "    m\n",
    "else:\n",
    "    say(\"latitude/longitude/price missing; skipping heatmap.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0caed46",
   "metadata": {},
   "source": [
    "### 6.2 Predicted vs Actual for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b97a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pick best regularized model by RMSE (from results_df)\n",
    "best_name = results_df.iloc[0]['model']\n",
    "say(f\"Best model from Section 4: {best_name}\")\n",
    "\n",
    "best_estimator = {'Lasso': gs_lasso, 'Ridge': gs_ridge, 'ElasticNet': gs_enet}[best_name]\n",
    "best_estimator.fit(X_train, y_train)\n",
    "preds = best_estimator.predict(X_test)\n",
    "\n",
    "plt.scatter(y_test, preds, alpha=0.4)\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title(f'Predicted vs Actual — {best_name}')\n",
    "lims = [min(plt.xlim()[0], plt.ylim()[0]), max(plt.xlim()[1], plt.ylim()[1])]\n",
    "plt.plot(lims, lims, '--')\n",
    "plt.xlim(lims); plt.ylim(lims)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e71f12",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad9d0bd",
   "metadata": {},
   "source": [
    "\n",
    "- Assumptions and deviations, if any.  \n",
    "- Data source and library versions.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}